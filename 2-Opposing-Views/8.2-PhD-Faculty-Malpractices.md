# Part 8.2

- **University PhD advisors**
- **University PhD committee members**
- **University CS department Chair**

## Chris Kanan

<p align="center">
<a href="https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Light/Kanan.png">
 <picture>
   <source width="65%" media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Dark/Kanan.png">
   <img width="65%" alt="Text changing depending on mode. Light: 'Light' Dark: 'Dark'" src="https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Light/Kanan.png">
</picture>
</a>
</p>

They Holocausted me.

They Holocausted me. By whatever legal definition exists, they Holocausted me. Every lawyer agrees, that by whatever legal definition exists, they Holocausted me.

Now rewind in time a bit. Not to the days of the Canaanites, but to the days before Chris Kanan.

Two of my advisors and two of my committee members deserted, for funsies, e.g. sabbaticals (I barely had time for Sabbath), promotions, retirement, funding reallocation, etc., and I was left with a brand new interim committee, consisting of Chenliang Xu, who had just announced his desertion, Charles Venuto, who worked in the Med Center and hadn't seen me in 3 years, Chris Kanan, last-minute added interim committee member, and Tom Howard, last-minute added interim committee member.

I never effectively met half of my committee, we had one 20-minute Zoom meeting call in which I presented, and then an after-room discussion.

In defense of the committee members who agreed on the decision, most of them didn't know me and hadn't met each other or me previously, and one of them, Chris Kanan, in a kind of *deus ex machina* (or so I thought, since that should have conclusively settled it), [mis-remembered](https://github.com/slerman12/BrokenWisdoms/blob/Ancillary/Deus-Ex-Memory.md) the short Zoom meeting that the four of us had (I highly recommend clicking on that link), on which these decisions from their end were based, as being 1.5 hours and twice. That is quite a lapse in memory (and Chris Kanan is the youngest). The meeting was 30 minutes, and once, as the other committee members may recall and confirm. The calendar invitation that that committee member, Chris Kanan, accepted, wrote: "Friday May 5, 2023 ⋅ 1pm – 1:45pm (Eastern Time - New York)" (extra lag time added). I have on email to one committee member Tom Howard that we planned half an hour: "Would you be available any of the following dates for a half-hour remote 6-month review?" It can be confirmed that it was 30 minutes in which I had to summarize 6 years of work over Zoom: 20ish minutes spent on my presentation, then 5ish minutes for their independent-room discussion and 5 minutes for their concluding remarks. 

### UnifiedML

> To do

It's fascinating how this gets linguistically obstructed whenever I try to write about it, but it was the majority of the work

### Novel Reinforcement Learning Systems Algorithm Suggestion

I should add details about the work that Chris Kanan asked me to do, or suggested rather, that I followed through on. Maybe in this [chapter](3-Disproof.md) and [this one](6-Indebted.md), but see terminator-bubble-diagram immediately above and I'll summarize below.

Long-term memory is what I was working on longest. From the Spring semester of my Freshman year, 
- when I built a long-term episodic memory reinforcement learning controller on top of image features from Felzenshwalb segmentation in Chenliang Xu's machine vision class,
- to that semester where I actually did meet Tom Howard for 5 minutes and told him about using the idea for robotics when we went to his robotics lab because he took an interest (and that short exchange is the only time I talked to him prior to this),
- to email-record (e.g. with Prof. Len Schubert),
- as well as my area exam,
- as well as my thesis proposal,
- as well as what I was doing at the Med Center that Charles Venuto has email record and maybe memory of,
- up to even this meeting with the new committee, where I showed [all of what I had done on that (lifelong, adaptive, rewritable, growing, accelerated Replay Memory for use in RL or supervised learning or generative modeling) and more](https://docs.google.com/presentation/d/1ZQFeiOKSU2teVGg5EHlgREAi8pJY_Mr1CJ3H2pfFBfI/edit?usp=sharing).

Coincidentally, they all at least had evidence that this is something I was trying to do through my full PhD, and *for some reason* — I'll let you guess why — maybe didn't get to. Everybody wanted me to pursue their different independent area research. Except Chris Kanan. Who suggested, wisely, after that 20-minute Zoom meeting, by email one month after I delivered a big new research proposal as they insanely requested, that I develop a brand new reinforcement learning systems algorithm for my giant and novel built-from-scratch deep learning framework [UnifiedML](https://github.com/slerman12/Builder), and I did within two months ([here is one of the commits](https://github.com/slerman12/Builder/commit/e8b896c75bdeb388995af912a25108dfa16a92f9) when I finished implementing [it, sampling without replacement within a priority, accelerated, parallelized, hard-disk-memory-mappable Replay Memory using truly-shared RAM memory, dynamically growable and rewritable and unified with supervised and generative modeling](https://github.com/AGI-init/UnifiedML/blob/ef14f7ff14b0b494d14bc8ee4bfe8925ad2a1fb3/ML/World/Replay.py#L566-L607), innovating on the state of the art reinforcement learning algorithm/system DrQV2, which very much does not do the vast majority of those things, without loss in generality), the day before the meeting with Chenliang, when I told him about my algorithm/system in [my UnifiedML framework](https://docs.google.com/presentation/d/1ZQFeiOKSU2teVGg5EHlgREAi8pJY_Mr1CJ3H2pfFBfI/edit?usp=sharing) — and it actually is brilliant — and he liked it dismissively, then proceeded to tell me he couldn't be my advisor because I didn't have enough papers. <!--It was a good meeting, but he didn't know what he was doing.-->

### Sampling Without Replacement Novel RL Systems Algorithm Implemented in General Form in 2 Months 

> To do: Summarize this without out-of-context linguistics

In supervised learning, it's standard to sample from the dataset without replacement. This is easy to do for large datasets by parallelizing, multiple CPU workers pulling data from a fixed dataset.

> bad linguistics start here:
>
> That isn't the case for reinforcement learning, where the dataset is online, meaning it grows, it changes, data can be added, updated, or deleted. This also isn't necessarily easy to do, with parallelization, in curriculum learning. 
>
> Partitioning a changing dataset across multiple parallel devices, and keeping track of samples across all of them such that custom dataset-wide sampling algorithms, like the simple and novel sampling-without-replacement-in-RL one I made, so that they do not conflict, and keeping that efficient, using truly-shared RAM memory and memory mapping, was a big unsolved problem for me to come up with, solve, and fully program in 2 months.
>
> I built the first parallelized sampler for reinforcement learning that does sampling without replacement, which is algorithmically better in supervised learning. It supports curriculum learning, and is part of a concrescent framework supporting also generative modeling, real-time online or offline RL, and every other training paradigm. It also supports custom sampling methods, definable as if one were defining for an unparallelized framework.
>
> It's a paralellization and training system that is built to be usable and used for any, general-purpose use.
>
> In reinforcement learning, this is a big change from the baseline algorithm, DrQV2, since that algorithm requires some minimum number of workers, and doesn't generalize to no-parallelization, nor support different sampling strategies with parallelization. Mine does all of that, with no loss in performance. It's a huge amount of systems work to build a new parallelization algorithm.
>
> I more than satisfied Chris Kanan's request for a novel RL systems algorithm. The algorithm itself, "sampling without replacement," is novel for RL too and has many challenges, like the added data needing new indices across all of the devices, such that any device can sample from those new indices, pulling that data efficently without conflict.
>
> To do: write this without Michael Scott in my head causing the writing to be much stupider than me and "not explain clearly how big the contributions were, even for a long timespan." (...and explain clearly)
>
> Wernicke's and Broca's area create systems of actual opposition in a person's own linguistics. In this case, Psyche is very likely the linguistic. Or the context of GitHub. But most likely Michael.

### Memory Systems

It is the first rewritable DataLoader that serves as a Replay for supervised, generative, and reinforcement learning as well as the novel algorithm for sampling without replacement, something that is standardly done in the former two but had never been done for priority experience replays in the latter, especially not in combination with lifelong storage capacity via [dynamic device allocation](https://github.com/slerman12/Builder/blob/64c92d7bf5c09642e161548c36625739884b01f7/UnifiedML/src/ML/World/Memory.py#L103-L107), [hard disk memory mapping](https://github.com/slerman12/Builder/blob/64c92d7bf5c09642e161548c36625739884b01f7/UnifiedML/src/ML/World/Memory.py#L524-L543), and [truly-shared in-RAM adaptive memory](https://github.com/slerman12/Builder/blob/64c92d7bf5c09642e161548c36625739884b01f7/UnifiedML/src/ML/World/Memory.py#L59-L150), in other words though extremely reductive: an accelerated parallelized DataLoader whose contents can be changed by the agent, and which include generalized sampling methods even when the dataset size is growing such as in a priority experience replay in reinforcement learning for DataLoader, and all of this added with a truly state of the art reinforcement learning algorithm/system DrQV2 as well as others with support for continuous and discrete domains, online or offline learning, and on-policy or off-policy, all of which — yes, all of which — I generalized into a unified API and framework to fully support supervised, generative, and reinforcement learning, the most general DataLoader/Experience Replay/Memory ever built. 

Nothing like it exists — I'm afraid PyTorch or somebody will try to copy it upon reading this. The value of it was so immense and this brand new interim committee — all except Chris Kanan — had not the faintest scent of that — well, that's not true, Chenliang thought I could start a business and just cared about papers, but note, I needed his GPUs to finish. By the way, pretty much fully implemented, [and elegantly so](https://github.com/slerman12/Builder/blob/main/UnifiedML/src/ML/World/Memory.py) (but with no GPUs to finalize the small amount of work left on the code as a whole, nor funding or undestroyed-human-spirit left to write the immense amount of documentation still needed, and being that I was terminated, being unpaid, and all of the injustices, I think this book takes priority, not to mention, I've released all the code open-source under the MIT license, so traditional funding routes aren't an option anyway<!--, plus I have other interconnected plans for my research, if given the reigns/money-rains to make it happen)-->. 

Since I started building it for my PhD (from about 2021, in Chenliang's lab, onwards), some similar works have come out, like [PyTorch's entire reinforcement learning library](https://github.com/pytorch/rl), but none compare (the system was being built within a [GATO](https://arxiv.org/abs/2205.06175)/[JEPA](https://arxiv.org/abs/2301.08243)-type framework before either of those were published/announced). And has an [interface that plug-and-play substitutes the normal PyTorch DataLoader](https://github.com/slerman12/Builder/blob/main/UnifiedML/src/ML/World/Replay.py). It's also accelerated and faster in every substantive way for supervised, too, at least than Pytorch's default DataLoader, but has all of the groundwork for the more intricate accelerators (while serving *all* the aforementioned purposes), and was the first infrastructure in Chenliang Xu's Computer Vision lab [capable of training ImageNet efficiently from scratch](https://github.com/slerman12/Builder/blob/main/tributaries-ml/src/tributaries/Examples/Sweeps/ImageNet.py), something not one other student in his Computer Vision lab — even with the most published papers — had (and probably "has" as of time of writing) ever done. To be clear, I was the first student in his PhD lab's history — officially ranked one of the [top in the world in Computer Vision](https://csrankings.org/#/fromyear/2020/toyear/2024/index?vision&us) — to use any infrastructure (his or otherwise) for training ImageNet from scratch (meaning, not via fine-tuning an ImageNet-pre-trained model, nor just using a subset of the ImageNet dataset), the gold standard of his field, all within a GATO/JEPA type generalized (and more general than either of them) framework (whose construction also preceded both of those, by the way). That being said, his students, perhaps more so than Chenliang, appreciated what I was doing, though Chenliang knew it could've been valuable, and was good enough for a PhD. Both he and Niaz said I was ready to defend. Chenliang wanted me to write my thesis before he cut his funding, yet again on an impossible deadline. But a thesis is more important than a conference paper, and I couldn't have.

### B-, or "Hasn't Been Put Through Enough Systems"

I had to retake a grad-level systems class, on account of getting a *tiny fraction of a point* below B, and the professor, having decided my research wouldn't independently involve enough systems work, chose to not give me that tiny fraction of a point, so I ended up with a B-, the CS department PhD program requires a B or higher, and I had to take a whole semester <!--, in my 3rd year, during benzo recovery, --> of grad-level systems again, systems being my least favorite subfield of CS, and ended up mostly teaching the class anyway since it had to do with a topic that would, later, end up being taken over by AI. Chris Kanan, having suggested I implement a novel RL systems algorithm for my research, and me actually having done so in a 2 month time span, a huge, insane amount of work, disproved that earlier professor, and puts the school in debt for my retaking a grad-level systems class, and the teaching I did for it.

## Tom Howard

<p align="center">
<a href="https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Light/Tom-Howard.png">
 <picture>
   <source width="55%" media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Dark/Tom-Howard.png">
   <img width="55%" alt="Text changing depending on mode. Light: 'Light' Dark: 'Dark'" src="https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Light/Tom-Howard.png">
</picture>
</a>
</p>

Tom Howard's field is robotics.

The independent-room discussion reasonably marked the end of my portion of the meeting, or so I thought. However, after the independent-room discussion, we kept talking, I left, and Chenliang later told me that the committee members thought I dropped the line early. That was a misunderstanding. I really thought the meeting was over since they conveyed everything I had to do next, or at least that they were going to in that discussion. At the very, very end of the discussion, one of the committee members, Tom Howard, said, "I just want to add, I think it's important." I actually in my sleep deprivation thought he meant my work (I suspect he probably meant the chapter-list), so I did a sincere and prolonged "bless you" sign with my hands in the silence and left. I know that this is the least believable part. Chenliang knew via Slack I hadn't slept that whole night, due to insomnia and maybe-nervousness, and I mentioned that "I didn't sleep all night" during the meeting to which I think Tom Howard off-handedly, maybe-non-noticedly said "I know," before resuming his point. 

Chenliang had to take his dog to the vet, for that matter. Either his wife did or him, but the meeting had to be no more than half an hour by what he told me. How we scheduled, over Slack. 
- If I left early, it's further justified by what Chenliang wrote to me on Slack, about needing to take his dog to the vet immediately after the meeting, assumed 30 minutes, his wife possibly being able to do it, or otherwise having to reschedule the vet to later in the day, all of which probably can be verified with either his vet or his wife or just his calendar.

He (Tom Howard) concluded the meeting saying "I just want to add, I think it's important" in reference to the chapter-list, and I did a "bless you" sign with my hands, as I'd on previous occasion felt the need to do after a robotics-related pitch to Chenliang's group coincidentally, in-person in Chenliang's lab, and robotics is Tom Howard's field. That was a coincidence though. It was just a vivid moment in Chenliang's lab, with all of his students there as witnesses, when he agreed to buy me one of the $300 Bittle robots (a friend of mine in his lab later wanted one too) that I could use with the robotics software that I [built](https://github.com/AGI-init/tributaries/blob/main/Examples/Sweeps/Bittle.py), that I did a "bless you" sign for in gratitude (he ordered two, and I was building UnifiedML). I meant it in a similar way to Tom Howard, but I interpreted "I just want to add, I think it's important" as being in reference to my research, not the chapter-list.

The chapter-list was the only requirement they wanted to make in that meeting. We had the entire after-independent-room discussion and they didn't specify anything other than the chapter-list. I'm sure they weren't going to. Perhaps one of them can even confirm that nothing more than the chapter-list was discussed as extra requirement in their independent-room meeting. That was clear from context.

But afterwards, I got an email — I think from Chenliang and perhaps from the graduate coordinator — specifying two altogether different requirements, with only an auxiliary mention of the chapter-list. Here they were, as apparently decided by the committee (post-hoc) after their chapter-list decision in the actual six-month review:

> ### $\Huge &#8220;$
> As remediation, the committee asks the student to
>
> 1.   before June 5, 2023, provide a 1-page write up that addresses the problem statement, hypothesis, and approaches of the thesis and give an outline of thesis chapters,
> 2.    before August 30, 2023, re-do and pass a six-month review.
> ### $\Huge &#8221;$

It's ironic that I'm being asked for remediation.

I don't know who decided these extra requirements, if it was the CS chair Michael Scott, or Tom Howard, or Chris Kanan, or even Chenliang.

I delivered a 6-page write up, as well as a chapter-list, as well as a 1-page write up summary in the 6-page write up, meeting all of the requirements of (1) for this brand new interim committee that effectively never met me. 

Subsequently, Chris Kanan suggested that I innovate a new reinforcement learning systems-based algorithm (also suggesting it post-hoc), which I did, as I described [above](#chris-kanan).

These are enormous requirements that I met on their behalf.

Tom Howard didn't make any tangible, explicit, or otherwise suggestions, nor did my advisor or Charles. (The committee consisted of 4 people, my advisor Chenliang, Chris Kanan, Tom Howard, and Charles).

The actual six-month review was **May 5th, 2023**. Then Chenliang notified me of his desertion on **July 24th, 2023**, a day after I finished implementing the [novel reinforcement learning systems-based algorithm](https://github.com/AGI-init/UnifiedML/blob/ef14f7ff14b0b494d14bc8ee4bfe8925ad2a1fb3/ML/World/Replay.py#L566-L607) (and [here](https://github.com/slerman12/Builder/commit/e8b896c75bdeb388995af912a25108dfa16a92f9) is the specific commit), and Chenliang confirmed by email the desertion for the rest of the committee and Michael Scott on **August 10th, 2024**, the same day as my [early in-advance-of notice](https://github.com/slerman12/BrokenWisdoms/blob/Ancillary/Not-Enough-Carbs.md) to the committee and Michael Scott that, on account of that, I wouldn't attend the requirement (2), notified to them without their "no" prior to the **August 30, 2023** date, with Chenliang even specifying that I was ready to defend a thesis within the next few months and he could support me through November (note: I was terminated in October).

### Does no "no" mean no?

What do I mean by 'without their "no"'? I mean, I told them I wasn't going to make the make-up meeting, and gave them the reason why. They didn't reply affirmatively or negatively. As my employers, given one-month's advance notice, it's their responsibility to reply "no," or otherwise, no news is good news in the workplace, and missing that meeting, as I'd warned in advance I would do and had more than enough justification, cannot be used as the termination reason, let alone pretty-much-sole termination reason<!-- not disproven by the information provided them-->. However, even if it is, that will later be addressed too, in accord with their own policy.

Also, the meeting itself was based on their false premise, that I hadn't satisfied what was requested in the six-month review. And it is ridiculous to demand a make-up for a six-month review where you are a brand new committee. Just wait six months, especially as the student is finding a new advisor for the second time in his sixth year through no faut of his own!

They ignored so many of my emails, including the major descriptive ones.

The truth is, these academics want to be seduced (e.g., "flying colors") or given courtesies (even when it's at the huge sacrifice cost of the employee). They don't care about research quality, and, as will be shown later, their own policy, by which they justify their employee injuries and termination.

...especially on a notified inaction, employer's requiring a student telepathically empathize with them isn't a thing, when information went far beyond one's way to be communicated—no "no" is not a no.

Wait, but what about affirmative consent?

This is the workplace!

Well, isn't affirmative consent—

Shut up—this is the workplace! This isn't courtship or a romantic encounter. These are my employers, *employers*. I'm the employee. They're the ones wanting "affirmative consent" for an action I notified them of 3 weeks in advance by email. For an inaction. That would be like if a woman said, "I'm not coming to your house," and her powerful boss failed to respond "No," and as a result she was fired.

1. 3 week's advance notice from employee. <!--(after advisor deserts)-->
2. With a *really* good provided excuse<!--, due to one of the employers in the first place-->. <!--(second advisor deserted)-->
3. And as will be shown later, the employer violated the policy handbook that requires the action.
4. And the action itself, in the employee's case, wasn't justified. <!--(advisor writes in reply that student is ready to defend)-->

### Real-Time RL Robotics

This is all on top of UnifiedML.

In case the detail was missed, since Tom Howard's field is robotics, UnifiedML supports [real-time RL robotics](https://github.com/AGI-init/tributaries/blob/main/Examples/Sweeps/Bittle.py). It was one of the [first works to start to do so, before the idea gained popularity](https://github.com/PetoiCamp/OpenCat/issues/30). It was one of many [contributions](https://github.com/slerman12/Builder/blob/main/UnifiedML/src/ML/World/Environments/Bittle.py) and innovations I had to quickly go through in our one and only meeting with this interim group.

### Description

First I wasn’t able to do the robot tracking with [YOLO V7](https://arxiv.org/abs/2207.02696) and then [YOLO V8](https://arxiv.org/abs/2305.09972), because those cannot recognize the Bittle robot for object detection, and had to do the Bittle robot object detection [by sticking in a fork](https://github.com/alder-tree/Assets/blob/main/BrokenWisdoms/Bittle-YOLOv7.png).

Then I [got it to work](https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Bittle-Grounding-DINO.png) with [GroundingDINO](https://arxiv.org/abs/2303.05499), a foundation model that UnifiedML [supports the quick use of](https://github.com/slerman12/Builder/blob/main/UnifiedML/src/ML/Agents/Blocks/Architectures/Vision/FoundationModels/GroundingDINO.py), and [which wasn’t entirely easy to integrate](https://github.com/continue-revolution/sd-webui-segment-anything/issues/162), which also allows the geeralization of the method to arbitrary robots. Then a transform in UnifiedML allowed the reward optimization of the robot's progression across the screen by its pinpointed physical coordinates through GroundingDINO. The one caveat to this simplistic reward structure is the camera has to be held still, and I still had to program the pipeline beyond [generalized transforms](https://github.com/slerman12/Builder/blob/ef6f8909a9a66ff41f4f7d7d962f84b9bb0d93fe/UnifiedML/src/ML/World/Environment.py#L37) (and [here](https://github.com/slerman12/Builder/blob/ef6f8909a9a66ff41f4f7d7d962f84b9bb0d93fe/UnifiedML/src/ML/World/Replay.py#L159)) to allow camera position "resets" when the robot reaches the rightmost edge of the screen. While this methodology had been shown [very recently](https://docs.google.com/presentation/d/1Zm-uNjS6D80Oi44kSVuX-6VHBAvCYBnAYh0541oU4nM/edit?usp=sharing) to that time period to work for gyroscopic and accelerometer data, both of which the Bittle provides and [I extracted](https://github.com/slerman12/Builder/blob/main/UnifiedML/src/ML/World/Environments/Bittle.py) for use in the training ([wasn't easy](https://github.com/PetoiCamp/OpenCat/issues/30)) and made that extraction automatic for new users who just buy a Bittle, one of the main research questions is whether this strategy could work for holocentric vision, which it should since I extract the exact physical coordinates with GroundingDINO, thereby reducing the representation space and not adding much additional tensor data for the reinforcement learning training beyond the gysroscopic and accelerometer data.

In both vision model cases, it was a full <!--end-to-end--> framework for livestreaming data in real-time [from Youtube](https://github.com/slerman12/Builder/blob/main/UnifiedML/src/ML/World/Datasets/YouTube.py) ([implemented with VidGear](https://github.com/slerman12/Builder/blob/main/UnifiedML/src/ML/World/Datasets/YouTube.py) for supervised/demo-learning [and CamGear](https://github.com/slerman12/Builder/blob/main/UnifiedML/src/ML/World/Environments/YouTube.py) for real-time RL), integrated into the [fast state of the art parallel memory system described above](#memory-systems), filmed live from a smartphone, in a democratized setup that anyone can use, the first robotics infrastructure that one can pick up quickly, with an affordable robot, and watch the robot learn in real-time, indoors or outdoors, ideally such that even a child can [afford](https://www.petoi.com/products/petoi-robot-dog-bittle-x-voice-controlled?utm_source=googleads&utm_medium=cpc&utm_campaign=Robotics-BittleX-PMax&utm_term=&utm_content=&gad_source=1&gbraid=0AAAAACJ6OErS_eDYe8R878GQC6Qvnhaii&gclid=CjwKCAiAmfq6BhAsEiwAX1jsZ4eTl3TUdSoS-Ghns6vNBs6NAkDpzdVQYQT0ZJjSew1A7oresxdL5BoCqkoQAvD_BwE) and train, where ordinarily robot quadrupeds in robotics setups [in the literature](https://www.science.org/doi/10.1126/scirobotics.abk2822) had cost $20,000 or more, and a serious one that affords scalable (including for multi-agent), online parallelized memory, integration with other modes of learning, and real-time RL from YouTube livestreams for adaptive robotics setups from [Bittle](https://github.com/PetoiCamp/OpenCat/issues/30) robots to any other, like the [$74,500 Spot robot](https://robotsguide.com/robots/spot#). Sans YouTube livestreaming (e.g., from a smartphone), the simplicity, and a general-purpose open-source framework and scalable memory system, this strategy has become [more popular](https://youtu.be/Kf9WDqYKYQQ?si=QxJ2SkD1gvP2JzFh) since my termination.

By using Bittle, at least at the time the first to do so for real-time real-world RL, I made the first truly democratizable, affordable and accessible, and scalable <!--end-to-end-->robotics trainer.

I was still making this progress after the six-month review and before what would’ve been the next six-month review. The formal termination date was 5 months after the six-month review, 3 months after Chenliang's desertion, 2 months after the would-be make-up review. In that time, I updated the committee as much and about everything that I could’ve, given the materials and work they requested. I reported to Chenliang literally-almost everything, and he and his entire lab were aware of all of these plans for at least a year, and six-months is how long PhD students usually get to report back to a committee, and usually with security under the grad school providing wage and advisor. 

### Art of Science Competition

It might look like the most trite, but I even submitted the Bittle work<!--I submitted a Bittle related artwork--> to the [University of Rochester Art of Science Competition](https://docs.google.com/presentation/d/13tAT_JixcAt1-ym_0Gcw4yZ93siCoKRCwuTxYoEFWUA/edit?usp=sharing) (as a still image), asking for Michael's advice on **03/13/23** by email, 2 months before the six-month review with the brand new committee, since he headed the CS department and no student in the University of Rochester CS department had ever won the Art of Science Competition. I showed Michael 4 different artworks. Michael advised against all of them as such:

> ### $\Huge &#8220;$
> Hi Sam – 
> 
> I’m pretty sure you can find winners of past competitions online. They are mostly NOT technical diagrams but rather images that look like "real" art but come from scientific pursuits.
>
> – Michael
> ### $\Huge &#8221;$
> &ensp;&ensp;- **03/13/2023**

My original planned submission candidates were technical, but beautiful diagrams, having to do with the intricates of the UnifiedML framework, that I'm more proud of than the new "Bittle Bots" submission artwork I made instead, after their feedback.

Chenliang proposed the submission I went with, and I made it quickly afterwards:

> ### $\Huge &#8220;$
> I agree with Michael. 
>
> Sam: one idea is to tell a story (in a picture) of the Bittle Bots trained by your UnifiedML.
> ### $\Huge &#8221;$
> &ensp;&ensp;- **03/13/2023**

So I did, submitting the [linked](https://docs.google.com/presentation/d/13tAT_JixcAt1-ym_0Gcw4yZ93siCoKRCwuTxYoEFWUA/edit?usp=sharing) picture story (as a still image).

This shows that Chenliang was very supportive about my "Bittle Bots" work, even just a couple of months before the six-month review, though perhaps I disagree that technical diagrams aren't "'real' art." The 4 I wanted to submit were these, as follows, my favorites being, in order, "Framework," "Jungly," "Squigglies," and last, the 4th one (that I called "Public Library") was actually also proposed by Chenliang (to compare to other works), but earlier independent the Art of Science Competition, in December, 2022, less than 6 months before the 6 month review. 

- ["Framework"](https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Framework.png)
- ["Jungly"](https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Jungly.png)
- ["Squigglies"](https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Squigglies.png)
- ["Public Library"](https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Public-Library.png)

I also included this caption:

> ### $\Huge &#8220;$
> "UnifiedML is a Machine Learning bow and arrow that bullseyes-and-destroys nukes, armored tanks, and flying saucers while perched reverently in muddy camouflage on Earth’s green trees."
>
> More philosophically, UnifiedML can be defined as an expanding ontology for the process of epistemology, however the expanding is in some way epistemelogical.
> ### $\Huge &#8221;$
> &ensp;&ensp;- **03/13/2023**

I presented these [other diagrams](https://docs.google.com/presentation/d/1ZQFeiOKSU2teVGg5EHlgREAi8pJY_Mr1CJ3H2pfFBfI/edit?usp=sharing), also elaborate technical diagrams, to try to summarize my work during my six-month review.

Well prior to that, another student set up [these training environments in Chenliang's lab]() [To do: add photos to main Github repo], hoping to work on the idea as well after I proposed it (after I had Chenliang order the bots by proposing to use the RL-based methodology)—and Chenliang had wanted to apply it to a fire-fighting proof-of-concept (like a fire-fighter dog, but now in retrospect, that's a good pun for after my being fired).

Chenliang, having recognized UnifiedML as being impressive by itself for a PhD student's thesis, not to mention the XRD project which was funding me, and everything else—the Actor Critic Creator framework more specifically—believed I was ready to defend and had seen that my progress on real-time real-world RL robotics was already on top of an insightful and useful framework, and before any of the related works had gained popularity. The new committee had not had the chance to engage as much with what I was doing, and most of them had abandoned ship as soon as the ambitiousness was even begun to be unpacked, and were already on lifeboats far away by the time the context of the information was redelivered and expanded-on through multiple emails after the six-month review to their email inboxes, still prior to the make-up review deadline.

Regarding these 4 artworks, Michael did respond with a "negative non-consent," and I adhered to his and Chenliang's recommendation, submitting the "Bittle Bots" picture story. It lost. It got rejected. I was there at the Art of Science Competition and there were many better submissions that deserved the democratic vote over mine. I walked out upset, but don't disagree.

### Petoi CEO

I even got a scheduled video meeting with the [Petoi](https://www.petoi.com/) CEO and access to the app [WeChat](https://en.wikipedia.org/wiki/WeChat), a Chinese app that requires invitation to join, with which to interact with him, or, my friends in Chenliang’s lab. 

Of course, he programmed the whole thing, and built it—Bittle I mean—amazing he answers Issues, emails, and offers video calls directly for troubleshooting, and he was helpful and patient as we/I got the Bittle bot to work after an initial failed setup by another one of Chenliang’s students, that they labeled with some name on a piece of masking tape (e.g., like "Andy") that I had to remove because I thought the robot was sacred, that it was the fore way to building a good robotics library. 

<!--
Be specific that his other student couldn’t even set up the robot, and maybe that their ideas were stupid and unrealistic to put on the shoulders of a 6th PhD student expected to write a thesis (with own plans), but work only on a meta level of humor, that I may call the “dis-world,” the land of the yin. 
-->

Generally applicable to more expensive and specialized robots, but accessible to everybody. Bittle understands the importance of pedagogy. I have no sponsorship, deals, or arrangements with Petoi. Not only do they provide just robotics lessons, an actually really good [crash course](https://www.yuque.com/tinkergen-help-en/bittle_course/lesson_1) that even teaches about "bionics" (when technology is learned from nature), and is provided, by him or somebody interestingly, (and, not only that) but it’s also meant to be easy for a child to use and learn, which is essential for the safety and transparency of the technologies that are likely to automate the future. 

I discuss more on this [here](4-Holy-Faith/3-Medical-Policies.md#mass-production-of-mass-production--anti-homogenization-and-monopolization). However, he doesn’t provide robotics training software, or software otherwise (had to rebuild, [simpler](https://github.com/AGI-init/UnifiedML/blob/main/ML/World/Environments/Bittle.py)), that necessarily generalizes to the same goal. That’s where I come in, or was coming in.

There was a synchronicity when I got access to that WeChat app. When I initially asked, the approval got rejected and my screen [was white](https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/WeChat-Yang.png), then we tried again—it was [black](https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/WeChat-Yin.png)—yin—in the screenshot sent to Chenliang’s Slack group that [it](https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Verification-Successful.png) worked. 

Much maybe what [I had to integrate](https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Group-1.png) (link to initial point image) in his group ([group photo with crooked hat](https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Group-2.png)) was [my dark side](https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Group-3.jpeg) (the last one maybe went a little too far, and [here is where I started from Henry](https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Henry-Group.jpeg)). Gravity, not just light.  The Word, not words. Or the theory and active practice discipline therein.

When we took that photo, over Chinese food in Chenliang’s lab, there was murmur of all being "synchronized," as I sat and listened. 

And walked a bit. That time not with my peers. Other times, good walk, in Chenliang’s trail behind his house, and got to pluck a pear from his own garden trees. 

I said, "Wow, if I had a tree like this, I would eat these all the time!"

One time, running, I kicked a soccer ball that Chenliang passed to me. I never scored in my life. I kicked it and shot it through the goalpost. I felt the sense of him to my left, and my kick was clumsy but didn’t land clumsy. The rest of his group was better. 

<!--### The Time My Apartment Flooded-->

I once shot a basketball—that I was good at—from half court as a kid. Shortest kid on the team, end seconds of the match, down 3 (wait, 2) points. It flew, and if it landed, that would’ve been the victory. I didn’t get the ball the whole game. It appeared in my hands then at the end and I threw. From half court—even professionals don’t make that. It shot. My mom was in the audience. The buzzer rang. The game was over. The ball flew, up over the court and dead center on the rim, bounced up, and curved. Just barely didn’t make it. But everyone in the audience knew I did. 

My family had a basketball hoop—4 or 5—in the courts further outside from our apartment, in the grassy field. Today they’re just cement. Were removed for some reason. Guess the apartment complex can’t bother to provide kids with playgrounds anymore—even the center playground is now just a fence. 

Meanwhile, they inspect our apartment every week—or so it feels, every year or some months, periodically. My mom keeps track. And they refine the carpet with a much better carpet—oh yes, and I’m being sarcastic, we had a nice carpet and they "refined" it with this grey coarse thing. Our kitchen sink broke several times due to the piping, kitchen flooded. We had to clean up neighbor food. 

This happened once after I was terminated, and the maintenance guy said it was our problem, and that it was fixed now. My mom was sure it wasn’t our problem and that evening it flooded again. The first time, shame on us. The second time, the guy came back and remarked that yeah, it was the piping. 

The University of Rochester might think I flooded their email inboxes with bad emails, trying to explain to them all of this. But they flooded those emails—the upstairs people of them, whose food ranked the floor of my kitchen. And I was the embarrassment, ignored and terminated. 

## Charles Venuto

<p align="center">
<a href="https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Light/Charles-Committee.png">
 <picture>
   <source width="44%" media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Dark/Charles-Committee.png">
   <img width="44%" alt="Text changing depending on mode. Light: 'Light' Dark: 'Dark'" src="https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Light/Charles-Committee.png">
</picture>
</a>
</p>

I have to elaborate more on Charles since he was on my six-month review committee.

Charles was still on my committee from the Med Center years, but was about to be replaced after this latest six-month review by my PI of the last several years [Niaz](#niaz-abdolrahim) (the work for whom was funding me), as she'd agreed and Chenliang advised.

Charles was my Med Center advisor. Charles and Henry both oversaw me in those first 3 years, before Henry's promotion meant I had to switch into Chenliang's lab and start an altogether new Computer Vision-branch focus of research, with an ultimatum to submit a top-venue paper that summer *in* Computer Vision, literally a brand new field to me. And I did. Amazingly. Though that seems to be unappreciated (the benchmarks and conventions for actual acceptance, though not necessarily principle of concept, are more elaborate than just plopping MHDPA on top of a CNN, had I done that). However, the [explainability-centric paper that I came up with, derived, wrote, and programmed](https://arxiv.org/abs/2006.08601) would not have been my preferred research area, since I was working on MHDPA and long-term episodic memory methods for Charles in the Med Center (and Henry) prior to that, and wrote [my novel MHDPA method paper](https://www.overleaf.com/read/qgmmzgsrctmg#6cd1b9) before MHDPA had any popularity or renown outside of small groups at DeepMind. MHDPA, for the uninitiated, became the backbone, 4 years later, of foundation models like ChatGPT. Henry doubted my obsession with the method back in 2018. No one at the university heard of it. And when switching to Chenliang's lab, he wasn't too thrilled with the idea of applying it to Semantic Image Segmentation, in what would've been perhaps (with some rarely-credited DeepMind experiments at the time having already done it on CNNs as the exception) the first Vision Transformer (since it would've had to convincingly beat baseline scores on Computer Vision benchmarks, which those DeepMind experiments hadn't applied the method on, and rarely are given the credit, despite unified principle of concept). So instead, under an employment-contingent ultimatum, I co-wrote my ICCV [Taylor-CAM](https://arxiv.org/abs/2006.08601) paper under Chenliang with Henry and Charles. Charles and I managed to sneak biomedical modeling into the appendix (see Appendix J, rather invisible).

I credited Charles as co-author on that paper, however they at the Med Center did not credit me for the prototype of the disease prognostication [model]((https://github.com/slerman12/DiseaseModeling)) (since developed since that original version that won in the PPMI contest, mentioned early [above](#charles-venuto)) and [website]((https://github.com/slerman12/DiseaseModelingWebApp)) I came up with, had to argue for, built, and presented in DC in front of renowned Parkinson's disease researchers, a poster and live demo presentation that also wasn't credited (and not to mention was still amid the very peak of my benzo withdrawal, that I had to take a benzo for to present).

I haven't spoken to Charles individually since we added that biomedical component to my [Taylor-CAM](https://arxiv.org/abs/2006.08601) paper and, 3 years later (and no hostility between us), he didn't say a word during the six-month review. I mean not a word, except — maybe — hi, and his video was off. That's 1/4 of the deciding committee, whose decision hinged on that meeting (on whose decision around that meeting determined my termination), who didn't have his video on or say a word during the deciding meeting. The other two were brand new interim replacements for other committee members who were absent. So that just leaves my advisor Chenliang, who didn't want to terminate me, who just wanted to reallocate my funding from the Mechanical Engineering group due to not enough papers in his Computer Vision group, and who believed my research was satisfactory enough to defend that month (of my termination), if I just "followed [his] formatting instructions" for the thesis that I hadn't started writing, that he thought I could write quickly.

So in other words: Chris Kanan (who [didn't remember the meeting](https://github.com/slerman12/BrokenWisdoms/blob/Ancillary/Deus-Ex-Memory.md)) and Tom Howard fully determined my termination, besides Michael L. Scott. 

## Recap

<p align="center">
<a href="https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Recap.png">
 <picture>
   <source width="75%" media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Recap.png">
   <img width="75%" alt="Text changing depending on mode. Light: 'Light' Dark: 'Dark'" src="https://raw.githubusercontent.com/alder-tree/Assets/refs/heads/main/BrokenWisdoms/Recap.png">
</picture>
</a>
</p>


